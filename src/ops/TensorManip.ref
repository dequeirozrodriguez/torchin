* TensorManip.ref - Tensor manipulation operations
*
* This module provides shape manipulation functions:
* - Reshaping: TReshape, TFlatten, TView
* - Transposing: TTranspose, TT, TPermute
* - Combining: TCat, TStack, TVStack, THStack
* - Slicing: TSlice, TIndex, TNarrow
* - Dimension ops: TSqueeze, TUnsqueeze, TExpand

%%
#include <torch/torch.h>
#include <vector>

// Access the tensor storage from TensorCore
namespace tensor_storage {
    extern std::unordered_map<int, torch::Tensor> tensors;
    extern int next_id;
    
    inline int store(torch::Tensor t) {
        int id = next_id++;
        tensors[id] = t;
        return id;
    }
    
    inline torch::Tensor* get(int id) {
        auto it = tensors.find(id);
        if (it != tensors.end()) {
            return &(it->second);
        }
        return nullptr;
    }
}

// Helper: read single int
inline bool read_int(refalrts::Iter& b, refalrts::Iter& e, int64_t& val) {
    if (refalrts::empty_seq(b, e)) return false;
    if (b->tag != refalrts::cDataNumber) return false;
    val = b->number_info;
    refalrts::move_left(b, e);
    return true;
}

// Helper: read list of ints
inline std::vector<int64_t> read_int_list(refalrts::Iter& b, refalrts::Iter& e) {
    std::vector<int64_t> result;
    while (!refalrts::empty_seq(b, e)) {
        if (b->tag == refalrts::cDataNumber) {
            result.push_back(b->number_info);
            refalrts::move_left(b, e);
        } else {
            break;
        }
    }
    return result;
}

#define GET_ONE_TENSOR(id, t) \
    refalrts::Iter content_b = 0, content_e = 0; \
    refalrts::call_left(content_b, content_e, arg_begin, arg_end); \
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible; \
    int id = content_b->number_info; \
    refalrts::move_left(content_b, content_e); \
    torch::Tensor* t = tensor_storage::get(id); \
    if (!t) return refalrts::cRecognitionImpossible;

#define RETURN_TENSOR(result) \
    int result_id = tensor_storage::store(result); \
    refalrts::reinit_number(arg_begin, result_id); \
    refalrts::splice_to_freelist(vm, arg_begin->next, arg_end); \
    return refalrts::cSuccess;
%%


*==============================================================================
* RESHAPING OPERATIONS
*==============================================================================

*==============================================================================
* <TReshape s.TensorID e.NewShape> == s.TensorResult
* Reshape tensor to new dimensions (total elements must match)
* Example: <TReshape t 2 6> reshapes a 3x4 tensor to 2x6
*==============================================================================
$ENTRY TReshape {
%%
    GET_ONE_TENSOR(id, t);
    
    std::vector<int64_t> new_shape = read_int_list(content_b, content_e);
    if (new_shape.empty()) {
        return refalrts::cRecognitionImpossible;
    }
    
    torch::Tensor result = t->reshape(new_shape);
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* <TView s.TensorID e.NewShape> == s.TensorResult
* View tensor with new shape (must be contiguous, shares memory)
* Use -1 for one dimension to infer its size
* Example: <TView t 3 -1> with 12 elements gives 3x4
*==============================================================================
$ENTRY TView {
%%
    GET_ONE_TENSOR(id, t);
    
    std::vector<int64_t> new_shape = read_int_list(content_b, content_e);
    if (new_shape.empty()) {
        return refalrts::cRecognitionImpossible;
    }
    
    torch::Tensor result = t->view(new_shape);
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* <TFlatten s.TensorID> == s.TensorResult
* Flatten tensor to 1D
* Example: <TFlatten t> where t is 3x4 gives tensor of 12 elements
*==============================================================================
$ENTRY TFlatten {
%%
    refalrts::Iter content_b = 0, content_e = 0;
    refalrts::call_left(content_b, content_e, arg_begin, arg_end);
    
    if (content_b->tag != refalrts::cDataNumber) {
        return refalrts::cRecognitionImpossible;
    }
    int id = content_b->number_info;
    
    torch::Tensor* t = tensor_storage::get(id);
    if (!t) return refalrts::cRecognitionImpossible;
    
    torch::Tensor result = t->flatten();
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* <TFlattenRange s.TensorID s.StartDim s.EndDim> == s.TensorResult
* Flatten dimensions from start_dim to end_dim
*==============================================================================
$ENTRY TFlattenRange {
%%
    GET_ONE_TENSOR(id, t);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int start_dim = content_b->number_info;
    refalrts::move_left(content_b, content_e);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int end_dim = content_b->number_info;
    
    torch::Tensor result = t->flatten(start_dim, end_dim);
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* TRANSPOSE OPERATIONS
*==============================================================================

*==============================================================================
* <TTranspose s.TensorID s.Dim0 s.Dim1> == s.TensorResult
* Swap two dimensions
* Example: <TTranspose t 0 1> swaps first two dimensions
*==============================================================================
$ENTRY TTranspose {
%%
    GET_ONE_TENSOR(id, t);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int dim0 = content_b->number_info;
    refalrts::move_left(content_b, content_e);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int dim1 = content_b->number_info;
    
    torch::Tensor result = t->transpose(dim0, dim1);
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* <TT s.TensorID> == s.TensorResult
* Transpose a 2D matrix (shorthand for <TTranspose t 0 1>)
*==============================================================================
$ENTRY TT {
%%
    refalrts::Iter content_b = 0, content_e = 0;
    refalrts::call_left(content_b, content_e, arg_begin, arg_end);
    
    if (content_b->tag != refalrts::cDataNumber) {
        return refalrts::cRecognitionImpossible;
    }
    int id = content_b->number_info;
    
    torch::Tensor* t = tensor_storage::get(id);
    if (!t) return refalrts::cRecognitionImpossible;
    
    torch::Tensor result = t->t();
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* <TPermute s.TensorID e.Dims> == s.TensorResult
* Permute dimensions according to the given order
* Example: <TPermute t 2 0 1> for a 3D tensor
*==============================================================================
$ENTRY TPermute {
%%
    GET_ONE_TENSOR(id, t);
    
    std::vector<int64_t> dims = read_int_list(content_b, content_e);
    if (dims.empty()) {
        return refalrts::cRecognitionImpossible;
    }
    
    torch::Tensor result = t->permute(dims);
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* <TContiguous s.TensorID> == s.TensorResult
* Return a contiguous tensor (copy if necessary)
*==============================================================================
$ENTRY TContiguous {
%%
    refalrts::Iter content_b = 0, content_e = 0;
    refalrts::call_left(content_b, content_e, arg_begin, arg_end);
    
    if (content_b->tag != refalrts::cDataNumber) {
        return refalrts::cRecognitionImpossible;
    }
    int id = content_b->number_info;
    
    torch::Tensor* t = tensor_storage::get(id);
    if (!t) return refalrts::cRecognitionImpossible;
    
    torch::Tensor result = t->contiguous();
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* DIMENSION OPERATIONS
*==============================================================================

*==============================================================================
* <TSqueeze s.TensorID> == s.TensorResult
* Remove all dimensions of size 1
* Example: tensor of shape [1,3,1,4] becomes [3,4]
*==============================================================================
$ENTRY TSqueeze {
%%
    refalrts::Iter content_b = 0, content_e = 0;
    refalrts::call_left(content_b, content_e, arg_begin, arg_end);
    
    int64_t id;
    if (!read_int(content_b, content_e, id)) return refalrts::cRecognitionImpossible;
    
    torch::Tensor* t = tensor_storage::get(id);
    if (!t) return refalrts::cRecognitionImpossible;
    
    torch::Tensor res;
    int64_t dim;
    
    // Check if there is a dimension argument left
    if (!refalrts::empty_seq(content_b, content_e) && read_int(content_b, content_e, dim)) {
        res = t->squeeze(dim);
    } else {
        res = t->squeeze();
    }
    
    int new_id = tensor_storage::store(res);
    refalrts::reinit_number(arg_begin, new_id);
    refalrts::splice_to_freelist(vm, arg_begin->next, arg_end);
    return refalrts::cSuccess;
%%
}

*==============================================================================
* <TSqueezeDim s.TensorID s.Dim> == s.TensorResult
* Remove dimension at given position if size is 1
*==============================================================================
$ENTRY TSqueezeDim {
%%
    GET_ONE_TENSOR(id, t);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int dim = content_b->number_info;
    
    torch::Tensor result = t->squeeze(dim);
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* <TUnsqueeze s.TensorID s.Dim> == s.TensorResult
* Add a dimension of size 1 at the given position
* Example: <TUnsqueeze t 0> on shape [3,4] gives [1,3,4]
*==============================================================================
$ENTRY TUnsqueeze {
%%
    GET_ONE_TENSOR(id, t);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int dim = content_b->number_info;
    
    torch::Tensor result = t->unsqueeze(dim);
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* COMBINING OPERATIONS
*==============================================================================

*==============================================================================
* <TCat s.Dim s.Tensor1 s.Tensor2 ...> == s.TensorResult
* Concatenate tensors along a dimension
* Example: <TCat 0 t1 t2> concatenates along first dimension
*==============================================================================
$ENTRY TCat {
%%
    refalrts::Iter content_b = 0, content_e = 0;
    refalrts::call_left(content_b, content_e, arg_begin, arg_end);
    
    refalrts::Iter list_b = 0, list_e = 0;
    // Essential: Check for brackets around the tensor list
    if (!refalrts::brackets_term(list_b, list_e, content_b)) return refalrts::cRecognitionImpossible;
    
    // FIX: Skip past the bracketed term using link_info
    content_b = content_b->link_info;
    refalrts::move_left(content_b, content_e);
    
    std::vector<torch::Tensor> tensors;
    while (!refalrts::empty_seq(list_b, list_e)) {
        int64_t tid;
        if (read_int(list_b, list_e, tid)) {
            torch::Tensor* ptr = tensor_storage::get(tid);
            if (ptr) tensors.push_back(*ptr);
        } else {
            // Skip non-integers if any
            refalrts::move_left(list_b, list_e);
        }
    }
    
    int64_t dim = 0;
    if (!refalrts::empty_seq(content_b, content_e)) {
        read_int(content_b, content_e, dim);
    }
    
    if (tensors.empty()) return refalrts::cRecognitionImpossible;
    
    torch::Tensor res = torch::cat(tensors, dim);
    int new_id = tensor_storage::store(res);
    
    refalrts::reinit_number(arg_begin, new_id);
    refalrts::splice_to_freelist(vm, arg_begin->next, arg_end);
    return refalrts::cSuccess;
%%
}

*==============================================================================
* <TStack s.Dim s.Tensor1 s.Tensor2 ...> == s.TensorResult
* Stack tensors along a new dimension
* Example: <TStack 0 t1 t2> stacks along new first dimension
*==============================================================================
$ENTRY TStack {
%%
    refalrts::Iter content_b = 0, content_e = 0;
    refalrts::call_left(content_b, content_e, arg_begin, arg_end);
    
    refalrts::Iter list_b = 0, list_e = 0;
    if (!refalrts::brackets_term(list_b, list_e, content_b)) return refalrts::cRecognitionImpossible;
    
    // FIX: Properly skip the bracketed list
    content_b = content_b->link_info;
    refalrts::move_left(content_b, content_e);
    
    std::vector<torch::Tensor> tensors;
    while (!refalrts::empty_seq(list_b, list_e)) {
        int64_t tid;
        if (read_int(list_b, list_e, tid)) {
            torch::Tensor* ptr = tensor_storage::get(tid);
            if (ptr) tensors.push_back(*ptr);
        } else {
            refalrts::move_left(list_b, list_e);
        }
    }
    
    int64_t dim = 0;
    if (!refalrts::empty_seq(content_b, content_e)) {
        read_int(content_b, content_e, dim);
    }
    
    if (tensors.empty()) return refalrts::cRecognitionImpossible;
    
    torch::Tensor res = torch::stack(tensors, dim);
    int new_id = tensor_storage::store(res);
    
    refalrts::reinit_number(arg_begin, new_id);
    refalrts::splice_to_freelist(vm, arg_begin->next, arg_end);
    return refalrts::cSuccess;
%%
}

*==============================================================================
* <TVStack s.Tensor1 s.Tensor2 ...> == s.TensorResult
* Vertical stack (along dimension 0)
*==============================================================================
$ENTRY TVStack {
%%
    refalrts::Iter content_b = 0, content_e = 0;
    refalrts::call_left(content_b, content_e, arg_begin, arg_end);
    
    refalrts::Iter list_b = 0, list_e = 0;
    if (!refalrts::brackets_term(list_b, list_e, content_b)) return refalrts::cRecognitionImpossible;
    
    // FIX: Properly skip the bracketed list
    content_b = content_b->link_info;
    refalrts::move_left(content_b, content_e);
    
    std::vector<torch::Tensor> tensors;
    while (!refalrts::empty_seq(list_b, list_e)) {
        int64_t tid;
        if (read_int(list_b, list_e, tid)) {
            torch::Tensor* ptr = tensor_storage::get(tid);
            if (ptr) tensors.push_back(*ptr);
        } else {
            refalrts::move_left(list_b, list_e);
        }
    }
    
    if (tensors.empty()) return refalrts::cRecognitionImpossible;
    
    torch::Tensor res = torch::vstack(tensors);
    int new_id = tensor_storage::store(res);
    
    refalrts::reinit_number(arg_begin, new_id);
    refalrts::splice_to_freelist(vm, arg_begin->next, arg_end);
    return refalrts::cSuccess;
%%
}

*==============================================================================
* <THStack s.Tensor1 s.Tensor2 ...> == s.TensorResult
* Horizontal stack (along dimension 1)
*==============================================================================
$ENTRY THStack {
%%
    refalrts::Iter content_b = 0, content_e = 0;
    refalrts::call_left(content_b, content_e, arg_begin, arg_end);
    
    refalrts::Iter list_b = 0, list_e = 0;
    if (!refalrts::brackets_term(list_b, list_e, content_b)) return refalrts::cRecognitionImpossible;
    
    // FIX: Properly skip the bracketed list
    content_b = content_b->link_info;
    refalrts::move_left(content_b, content_e);
    
    std::vector<torch::Tensor> tensors;
    while (!refalrts::empty_seq(list_b, list_e)) {
        int64_t tid;
        if (read_int(list_b, list_e, tid)) {
            torch::Tensor* ptr = tensor_storage::get(tid);
            if (ptr) tensors.push_back(*ptr);
        } else {
            refalrts::move_left(list_b, list_e);
        }
    }
    
    if (tensors.empty()) return refalrts::cRecognitionImpossible;
    
    torch::Tensor res = torch::hstack(tensors);
    int new_id = tensor_storage::store(res);
    
    refalrts::reinit_number(arg_begin, new_id);
    refalrts::splice_to_freelist(vm, arg_begin->next, arg_end);
    return refalrts::cSuccess;
%%
}

*==============================================================================
* SLICING AND INDEXING
*==============================================================================

*==============================================================================
* <TSlice s.TensorID s.Dim s.Start s.End> == s.TensorResult
* Slice tensor along dimension from start to end (exclusive)
* Example: <TSlice t 0 1 3> gets rows 1 and 2 (0-indexed)
*==============================================================================
$ENTRY TSlice {
%%
    GET_ONE_TENSOR(id, t);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int dim = content_b->number_info;
    refalrts::move_left(content_b, content_e);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int start = content_b->number_info;
    refalrts::move_left(content_b, content_e);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int end = content_b->number_info;
    
    torch::Tensor result = t->slice(dim, start, end);
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* <TNarrow s.TensorID s.Dim s.Start s.Length> == s.TensorResult
* Narrow tensor along dimension starting at start for length elements
*==============================================================================
$ENTRY TNarrow {
%%
    GET_ONE_TENSOR(id, t);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int dim = content_b->number_info;
    refalrts::move_left(content_b, content_e);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int start = content_b->number_info;
    refalrts::move_left(content_b, content_e);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int length = content_b->number_info;
    
    torch::Tensor result = t->narrow(dim, start, length);
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* <TSelect s.TensorID s.Dim s.Index> == s.TensorResult
* Select a single slice along a dimension (reduces dimensionality by 1)
* Example: <TSelect t 0 2> selects row 2 from a matrix
*==============================================================================
$ENTRY TSelect {
%%
    GET_ONE_TENSOR(id, t);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int dim = content_b->number_info;
    refalrts::move_left(content_b, content_e);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int index = content_b->number_info;
    
    torch::Tensor result = t->select(dim, index);
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* <TIndexSelect s.TensorID s.Dim s.IndicesTensorID> == s.TensorResult
* Select elements along dimension using indices tensor
*==============================================================================
$ENTRY TIndexSelect {
%%
    GET_ONE_TENSOR(id, t);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int dim = content_b->number_info;
    refalrts::move_left(content_b, content_e);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int indices_id = content_b->number_info;
    
    torch::Tensor* indices = tensor_storage::get(indices_id);
    if (!indices) return refalrts::cRecognitionImpossible;
    
    torch::Tensor result = t->index_select(dim, indices->to(torch::kLong));
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* <TGather s.TensorID s.Dim s.IndicesTensorID> == s.TensorResult
* Gather values along dimension according to indices
*==============================================================================
$ENTRY TGather {
%%
    GET_ONE_TENSOR(id, t);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int dim = content_b->number_info;
    refalrts::move_left(content_b, content_e);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int indices_id = content_b->number_info;
    
    torch::Tensor* indices = tensor_storage::get(indices_id);
    if (!indices) return refalrts::cRecognitionImpossible;
    
    torch::Tensor result = t->gather(dim, indices->to(torch::kLong));
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* SPLITTING
*==============================================================================

*==============================================================================
* <TSplit s.TensorID s.SplitSize s.Dim> == s.Tensor1 s.Tensor2 ...
* Split tensor into chunks of given size along dimension
* Returns multiple tensor IDs
*==============================================================================
$ENTRY TSplit {
%%
    GET_ONE_TENSOR(id, t);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int split_size = content_b->number_info;
    refalrts::move_left(content_b, content_e);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int dim = content_b->number_info;
    
    auto chunks = t->split(split_size, dim);
    
    refalrts::reset_allocator(vm);
    
    refalrts::Iter first = 0, last = 0;
    for (auto& chunk : chunks) {
        int chunk_id = tensor_storage::store(chunk);
        refalrts::Iter node;
        refalrts::alloc_number(vm, node, chunk_id);
        if (first == 0) {
            first = node;
        }
        last = node;
    }
    
    if (first) {
        refalrts::splice_evar(arg_begin, first, last);
    }
    refalrts::splice_to_freelist(vm, arg_begin, arg_end);
    return refalrts::cSuccess;
%%
}


*==============================================================================
* <TChunk s.TensorID s.NumChunks s.Dim> == s.Tensor1 s.Tensor2 ...
* Split tensor into specified number of chunks along dimension
*==============================================================================
$ENTRY TChunk {
%%
    GET_ONE_TENSOR(id, t);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int num_chunks = content_b->number_info;
    refalrts::move_left(content_b, content_e);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int dim = content_b->number_info;
    
    auto chunks = t->chunk(num_chunks, dim);
    
    refalrts::reset_allocator(vm);
    
    refalrts::Iter first = 0, last = 0;
    for (auto& chunk : chunks) {
        int chunk_id = tensor_storage::store(chunk);
        refalrts::Iter node;
        refalrts::alloc_number(vm, node, chunk_id);
        if (first == 0) {
            first = node;
        }
        last = node;
    }
    
    if (first) {
        refalrts::splice_evar(arg_begin, first, last);
    }
    refalrts::splice_to_freelist(vm, arg_begin, arg_end);
    return refalrts::cSuccess;
%%
}


*==============================================================================
* REPEATING AND TILING
*==============================================================================

*==============================================================================
* <TRepeat s.TensorID e.Repeats> == s.TensorResult
* Repeat tensor along each dimension
* Example: <TRepeat t 2 3> repeats 2x along dim0, 3x along dim1
*==============================================================================
$ENTRY TRepeat {
%%
    GET_ONE_TENSOR(id, t);
    
    std::vector<int64_t> repeats = read_int_list(content_b, content_e);
    if (repeats.empty()) {
        return refalrts::cRecognitionImpossible;
    }
    
    torch::Tensor result = t->repeat(repeats);
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* <TTile s.TensorID e.Reps> == s.TensorResult
* Tile tensor (like numpy.tile)
*==============================================================================
$ENTRY TTile {
%%
    GET_ONE_TENSOR(id, t);
    
    std::vector<int64_t> reps = read_int_list(content_b, content_e);
    if (reps.empty()) {
        return refalrts::cRecognitionImpossible;
    }
    
    torch::Tensor result = t->tile(reps);
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* FLIPPING
*==============================================================================

*==============================================================================
* <TFlip s.TensorID e.Dims> == s.TensorResult
* Flip tensor along specified dimensions
* Example: <TFlip t 0> flips along first dimension
*==============================================================================
$ENTRY TFlip {
%%
    refalrts::Iter content_b = 0, content_e = 0;
    refalrts::call_left(content_b, content_e, arg_begin, arg_end);
    
    int64_t id;
    if (!read_int(content_b, content_e, id)) return refalrts::cRecognitionImpossible;
    
    torch::Tensor* t = tensor_storage::get(id);
    if (!t) return refalrts::cRecognitionImpossible;
    
    refalrts::Iter dim_b = 0, dim_e = 0;
    // Check if dims are wrapped in brackets (e.g. (0 1))
    if (refalrts::brackets_term(dim_b, dim_e, content_b)) {
        // Use the inner range for reading ints
        // brackets_term sets dim_b/dim_e to the content INSIDE the brackets
        // We don't need to manually move content_b past it if we use the inner iterator
        // BUT for the outer flow, we must skip the bracketed term in content_b
        refalrts::move_left(content_b, content_e); 
    } else {
        // Assume loose dims if no brackets (fallback)
        dim_b = content_b;
        dim_e = content_e;
    }
    
    std::vector<int64_t> dims = read_int_list(dim_b, dim_e);
    
    torch::Tensor res = t->flip(dims);
    int new_id = tensor_storage::store(res);
    refalrts::reinit_number(arg_begin, new_id);
    refalrts::splice_to_freelist(vm, arg_begin->next, arg_end);
    return refalrts::cSuccess;
%%
}


*==============================================================================
* <TFlipLR s.TensorID> == s.TensorResult
* Flip tensor left-right (along last dimension)
*==============================================================================
$ENTRY TFlipLR {
%%
    refalrts::Iter content_b = 0, content_e = 0;
    refalrts::call_left(content_b, content_e, arg_begin, arg_end);
    
    if (content_b->tag != refalrts::cDataNumber) {
        return refalrts::cRecognitionImpossible;
    }
    int id = content_b->number_info;
    
    torch::Tensor* t = tensor_storage::get(id);
    if (!t) return refalrts::cRecognitionImpossible;
    
    torch::Tensor result = t->fliplr();
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* <TFlipUD s.TensorID> == s.TensorResult
* Flip tensor up-down (along first dimension)
*==============================================================================
$ENTRY TFlipUD {
%%
    refalrts::Iter content_b = 0, content_e = 0;
    refalrts::call_left(content_b, content_e, arg_begin, arg_end);
    
    if (content_b->tag != refalrts::cDataNumber) {
        return refalrts::cRecognitionImpossible;
    }
    int id = content_b->number_info;
    
    torch::Tensor* t = tensor_storage::get(id);
    if (!t) return refalrts::cRecognitionImpossible;
    
    torch::Tensor result = t->flipud();
    RETURN_TENSOR(result);
%%
}


*==============================================================================
* <TRoll s.TensorID s.Shift s.Dim> == s.TensorResult
* Roll tensor elements along dimension
*==============================================================================
$ENTRY TRoll {
%%
    GET_ONE_TENSOR(id, t);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int shift = content_b->number_info;
    refalrts::move_left(content_b, content_e);
    
    if (content_b->tag != refalrts::cDataNumber) return refalrts::cRecognitionImpossible;
    int dim = content_b->number_info;
    
    torch::Tensor result = t->roll(shift, dim);
    RETURN_TENSOR(result);
%%
}

/* ============================================================================
 * CAUSAL MASK (Attention Mask)
 * Creates a square matrix [Size, Size].
 * Upper triangle (future) = -inf
 * Lower triangle (past)   = 0
 * ========================================================================== */
$ENTRY TCausalMask {
%%
    refalrts::Iter content_b = 0, content_e = 0;
    refalrts::call_left(content_b, content_e, arg_begin, arg_end);
    
    int64_t size;
    if (!read_int(content_b, content_e, size)) return refalrts::cRecognitionImpossible;
    
    //creating matrix full of -infinity
    torch::Tensor mask = torch::full({size, size}, -INFINITY);
    
    //keeping upper triangle (diagonal + 1), set rest to 0.
    // Result: 
    // 0  -inf -inf
    // 0   0   -inf
    // 0   0    0
    // (Note: torch::triu zeroes out the *other* part, so we pass -inf tensor)
    mask = torch::triu(mask, 1);
    
    int new_id = tensor_storage::store(mask);
    
    refalrts::reinit_number(arg_begin, new_id);
    refalrts::splice_to_freelist(vm, arg_begin->next, arg_end);
    return refalrts::cSuccess;
%%
}

/* ============================================================================
 * UPDATE SLICE (In-Place)
 * Efficiently updates a portion of a tensor. Critical for KV Cache.
 *
 * <TUpdateSlice s.Dest s.Src s.Dim s.Start>
 * s.Dest: Target tensor (e.g. KV Cache)
 * s.Src:  Source data (e.g. New Token Key/Value)
 * s.Dim:  Dimension to write along (e.g. Sequence dimension)
 * s.Start: Starting index
 * ========================================================================== */
$ENTRY TUpdateSlice {
%%
    refalrts::Iter content_b = 0, content_e = 0;
    refalrts::call_left(content_b, content_e, arg_begin, arg_end);
    
    int64_t dest_id, src_id, dim, start;
    if (!read_int(content_b, content_e, dest_id)) return refalrts::cRecognitionImpossible;
    if (!read_int(content_b, content_e, src_id)) return refalrts::cRecognitionImpossible;
    if (!read_int(content_b, content_e, dim)) return refalrts::cRecognitionImpossible;
    if (!read_int(content_b, content_e, start)) return refalrts::cRecognitionImpossible;
    
    torch::Tensor* dest = tensor_storage::get(dest_id);
    torch::Tensor* src = tensor_storage::get(src_id);
    
    if (!dest || !src) return refalrts::cRecognitionImpossible;
    
    // Safety check: ensure we don't write out of bounds
    int64_t len = src->size(dim);
    if (start + len > dest->size(dim)) {
        // Fail silently or print error? For now, just return dest unmodified to avoid crash
        // Ideally: print error to stderr
        std::cerr << "RefTorch Error: TUpdateSlice out of bounds!" << std::endl;
    } else {
        // The Magic: In-place copy
        // dest[dim, start:start+len] = src
        dest->narrow(dim, start, len).copy_(*src);
    }
    
    // Return original ID
    refalrts::reinit_number(arg_begin, dest_id);
    refalrts::splice_to_freelist(vm, arg_begin->next, arg_end);
    return refalrts::cSuccess;
%%
}

/* ============================================================================
 * TOP-K MASKING
 * Sets all values except the largest K to -infinity.
 * <TTopKMask s.Logits s.K s.FilterVal>
 * s.K: Number of top elements to keep
 * s.FilterVal: Value to fill filtered elements (usually -inf)
 * ========================================================================== */
$ENTRY TTopKMask {
%%
    refalrts::Iter content_b = 0, content_e = 0;
    refalrts::call_left(content_b, content_e, arg_begin, arg_end);
    
    int64_t id, k;
    double filter_val = -INFINITY; // Default
    
    if (!read_int(content_b, content_e, id)) return refalrts::cRecognitionImpossible;
    if (!read_int(content_b, content_e, k)) return refalrts::cRecognitionImpossible;
    
    // Optional 3rd arg: filter value (if user wants 0.0 instead of -inf)
    if (content_b && content_b->tag == refalrts::cDataNumber) {
        filter_val = static_cast<double>(content_b->number_info);
        //note: passing -inf from Refal is hard, so we assume standard behavior 
        //if arg is missing, or user passes specific integer.
    }
    
    torch::Tensor* logits = tensor_storage::get(id);
    if (!logits) return refalrts::cRecognitionImpossible;
    
    if (k > 0 && k < logits->size(-1)) {
        
        auto topk = torch::topk(*logits, k, -1);
        auto values = std::get<0>(topk);
        
        auto cutoff = values.select(-1, k - 1).unsqueeze(-1);
        
        //creating a mask where logits < cutoff
        //setting those to -inf
        torch::Tensor result = torch::where(*logits < cutoff, 
                                          torch::tensor(filter_val, logits->options()), 
                                          *logits);
        
        int new_id = tensor_storage::store(result);
        refalrts::reinit_number(arg_begin, new_id);
    } else {
        //K invalid or larger than vocab? Return clone.
        int new_id = tensor_storage::store(logits->clone());
        refalrts::reinit_number(arg_begin, new_id);
    }
    
    refalrts::splice_to_freelist(vm, arg_begin->next, arg_end);
    return refalrts::cSuccess;
%%
}

/* ============================================================================
 * TOP-P MASKING (Nucleus Sampling)
 * Keeps the top tokens with cumulative probability >= P.
 * <TTopPMask s.Logits s.P> (P scaled by 1000, e.g. 900 = 0.9)
 * ========================================================================== */
$ENTRY TTopPMask {
%%
    refalrts::Iter content_b = 0, content_e = 0;
    refalrts::call_left(content_b, content_e, arg_begin, arg_end);
    
    int64_t id, p_int;
    if (!read_int(content_b, content_e, id)) return refalrts::cRecognitionImpossible;
    if (!read_int(content_b, content_e, p_int)) return refalrts::cRecognitionImpossible;
    
    torch::Tensor* logits = tensor_storage::get(id);
    if (!logits) return refalrts::cRecognitionImpossible;
    
    double p = static_cast<double>(p_int) / 1000.0;
    
    if (p <= 0.0 || p >= 1.0) {
        int new_id = tensor_storage::store(logits->clone());
        refalrts::reinit_number(arg_begin, new_id);
    } else {
        
        auto sorted = torch::sort(*logits, -1, true);
        auto sorted_logits = std::get<0>(sorted);
        auto sorted_indices = std::get<1>(sorted);
        
        auto cumulative_probs = torch::softmax(sorted_logits, -1).cumsum(-1);
        
        //creating mask for removal
        //removing tokens with cumulative probability > p (but keeping the first one that crosses threshold)
        //shifting mask right by 1 to always keep the first token
        auto sorted_indices_to_remove = cumulative_probs > p;
        sorted_indices_to_remove.narrow(-1, 1, sorted_indices_to_remove.size(-1) - 1)
                                .copy_(sorted_indices_to_remove.narrow(-1, 0, sorted_indices_to_remove.size(-1) - 1).clone());
        sorted_indices_to_remove.select(-1, 0) = false;
        
        //scaterring mask back to original indices
        auto indices_to_remove = torch::zeros_like(sorted_indices_to_remove).to(torch::kBool);
        indices_to_remove.scatter_(-1, sorted_indices, sorted_indices_to_remove);
        
        //applyoing -inf
        torch::Tensor result = logits->masked_fill(indices_to_remove, -INFINITY);
        
        int new_id = tensor_storage::store(result);
        refalrts::reinit_number(arg_begin, new_id);
    }
    
    refalrts::splice_to_freelist(vm, arg_begin->next, arg_end);
    return refalrts::cSuccess;
%%
}
