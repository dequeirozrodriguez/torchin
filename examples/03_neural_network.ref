* 03_neural_network.ref - Neural Network Training Example
*
* This example demonstrates:
* - Building a simple MLP (multi-layer perceptron)
* - Forward pass with activations
* - Loss computation (MSE)
* - Backward pass and gradient descent
* - Full Training Loop

$INCLUDE "RefTorch";

$ENTRY Go {
  = <Main>;
}

Main {
  = <Prout '=== RefTorch Phase 3: Neural Networks ==='>
    <Prout ''>
    <Example1-Activations>
    <Example2-LinearLayer>
    <Example3-ForwardPass>
    <Example4-TrainingStep>
    <Example5-MiniTrainingLoop>
    <Cleanup>;
}


*------------------------------------------------------------------------------
* Example 1: Activation Functions
*------------------------------------------------------------------------------
Example1-Activations {
  = <Prout '--- Example 1: Activation Functions ---'>
    <Prout ''>
    <DoActivations <TFromList (3) (1000 0 <Sub 0 1000>)>>; /* 1.0, 0.0, -1.0 */
}

DoActivations {
  s.T
    = <Prout 'Input:'> <TPrint s.T>
      
      <Prout 'ReLU (Max(0,x)):'>
      <PrintAndFree <TRelu <TClone s.T>>>
      
      <Prout 'Sigmoid (1/(1+e^-x)):'>
      <PrintAndFree <TSigmoid <TClone s.T>>>
      
      <TFree s.T>
      <Prout ''>;
}


*------------------------------------------------------------------------------
* Example 2: Linear Layer Initialization
*------------------------------------------------------------------------------
Example2-LinearLayer {
  = <Prout '--- Example 2: Linear Layer Init ---'>
    <Prout ''>
    /* TInitLinear returns W B (loose), not (W B) */
    <DoInitLinear <TInitLinear 3 2>>;
}

DoInitLinear {
  /* FIX: Removed parentheses around s.W s.B */
  s.W s.B
    = <Prout 'Weights (2x3):'>
      <TPrint s.W>
      <Prout 'Bias (2):'>
      <TPrint s.B>
      
      <TFree s.W>
      <TFree s.B>
      <Prout ''>;
}


*------------------------------------------------------------------------------
* Example 3: Forward Pass
*------------------------------------------------------------------------------
Example3-ForwardPass {
  = <Prout '--- Example 3: Forward Pass ---'>
    <Prout ''>
    <DoForwardPass 
      <TFromList (1 3) (1 2 3)>   /* Input (1x3) */
      <TInitLinear 3 2>           /* Layer (W, B) */
    >;
}

DoForwardPass {
  /* FIX: Removed parentheses around s.W s.B */
  s.X s.W s.B
    = <Prout 'Input:'> <TPrint s.X>
      
      /* Linear: Y = X @ W.T + B */
      <Prout 'Linear Output:'>
      <TPrint <TLinear s.X s.W s.B>>
      
      <TFree s.X>
      <TFree s.W>
      <TFree s.B>
      <Prout ''>;
}


*------------------------------------------------------------------------------
* Example 4: Single Training Step (Manual SGD)
*------------------------------------------------------------------------------
Example4-TrainingStep {
  = <Prout '--- Example 4: Single Training Step ---'>
    <Prout ''>
    <TResetOptimizerState>
    <DoTrainingStep 
       <TRequiresGrad <TFromList (1 1) (2)> 1>  /* Weight=2.0, needs grad */
       <TFromList (1 1) (0)>                    /* Bias=0.0 */
       <TFromList (1 1) (3)>                    /* Input=3.0 */
       <TFromList (1 1) (10)>                   /* Target=10.0 */
    >;
}

DoTrainingStep {
  s.W s.B s.X s.Y
    /* 1. Forward */
    , <TLinear s.X s.W s.B> : s.Pred
    , <Prout 'Prediction (2*3+0=6):'> : e.I1
    , <TPrint s.Pred> : e.I2
    
    /* 2. Loss (MSE) */
    , <TMSE s.Pred s.Y> : s.Loss
    , <Prout 'Loss (MSE(6,10)=16):'> : e.I3
    , <TPrint s.Loss> : e.I4
    
    /* 3. Backward */
    , <TBackward s.Loss> : e.I5
    , <Prout 'Gradient on W (should be approx -24):'> : e.I6
    , <TPrint <TGrad s.W>> : e.I7
    
    /* 4. Update (SGD) */
    , <TUpdateSGD s.W 100> : e.I8 /* LR=0.1 */
    , <Prout 'Updated W (2 - 0.1*-24 = 4.4):'> : e.I9
    , <TPrint s.W> : e.I10
    
    /* Cleanup */
    = <TZeroGrad s.W>
      <TFree s.W> <TFree s.B> <TFree s.X> <TFree s.Y>
      <TFree s.Pred> <TFree s.Loss>
      <Prout ''>;
}


*------------------------------------------------------------------------------
* Example 5: Mini Training Loop (Adam)
*------------------------------------------------------------------------------
Example5-MiniTrainingLoop {
  = <Prout '--- Example 5: Training Loop (Adam) ---'>
    <Prout 'Task: Learn to output 10 from input 1'>
    <Prout ''>
    <TResetOptimizerState>
    <DoTrainingLoop
       /* W=0.0, B=0.0 */
       <TRequiresGrad <TFromList (1 1) (0)> 1>
       <TRequiresGrad <TFromList (1 1) (0)> 1>
       <TFromList (1 1) (1)>   /* Input=1 */
       <TFromList (1 1) (10)>  /* Target=10 */
    >;
}

DoTrainingLoop {
  s.W s.B s.X s.Y
    = <TrainLoop 1 50 s.W s.B s.X s.Y>
      <Prout 'Final Result:'>
      <TPrint <TLinear s.X s.W s.B>>
      <TFree s.W> <TFree s.B> <TFree s.X> <TFree s.Y>;
}

/* Loop Logic */
TrainLoop {
  s.I s.Max s.W s.B s.X s.Y, <Compare (s.I) (s.Max)> : '+'
    = ; /* Done */

  s.I s.Max s.W s.B s.X s.Y
    /* Zero Grads */
    , <TZeroGrad s.W> : e.z1
    , <TZeroGrad s.B> : e.z2
    
    /* Forward */
    , <TLinear s.X s.W s.B> : s.Pred
    , <TMSE s.Pred s.Y> : s.Loss
    
    /* Log every 10 steps */
    , <Mod (s.I) (10)> : s.Rem
    , <LogIfZero s.Rem s.I s.Loss> : e.LogIgn
    
    /* Backward */
    , <TBackward s.Loss> : e.back
    
    /* Optimizer: Adam(lr=0.1, b1=0.9, b2=0.999, eps=1e-8) */
    , <TUpdateAdam s.W 100 900 999 1> : e.u1
    , <TUpdateAdam s.B 100 900 999 1> : e.u2
    
    , <TFree s.Pred> : e.f1
    , <TFree s.Loss> : e.f2
    
    = <TrainLoop <Add s.I 1> s.Max s.W s.B s.X s.Y>;
}

LogIfZero {
  0 s.I s.Loss = <Prout 'Step ' s.I ' Loss: ' <TItem s.Loss>>;
  s.X s.I s.Loss = ;
}


*------------------------------------------------------------------------------
* Helpers
*------------------------------------------------------------------------------
PrintAndFree {
  s.T = <TPrint s.T> <TFree s.T>;
}

Cleanup {
  = <TFreeAll> <Prout 'Done.'>;
}

/* Math Helpers */
Compare {
  (s.A) (s.B), <Sub s.A s.B> : '-' s.X = '-'; /* A < B */
  (s.A) (s.B), <Sub s.A s.B> : 0       = '0'; /* A == B */
  (s.A) (s.B) = '+';                          /* A > B */
}

Mod {
  (s.X) (s.Y), <Div s.X s.Y> : s.Q, <Mul s.Q s.Y> : s.M
    = <Sub s.X s.M>;
}
